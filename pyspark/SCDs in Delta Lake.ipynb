{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df2a0706",
   "metadata": {},
   "source": [
    "# Slowly Changing Dimensions in Delta Lake\n",
    "--notebook tested on my local Spark 3.0 installation, as well as in Databricks Community Edition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0db028",
   "metadata": {},
   "source": [
    "SCD's (Slowly Changing Dimensions) are fairly easy to implement with Spark 3.0 and Delta Lake.\n",
    "In the traditional data warehousing world, where everything is stored in SQL tables, we used to have a construction like the below (from Kimball's [Data Warehouse toolkit](https://www.bookdepository.com/The-Data-Warehouse-Toolkit-Ralph-Kimball/9781118530801?redirected=true&utm_medium=Google&utm_campaign=Base1&utm_source=GR&utm_content=The-Data-Warehouse-Toolkit&selectCurrency=EUR&w=AFFMAU9SYY661SA8V9F5)) for a Type 2 SCD:\n",
    "\n",
    "![SCD2 Example](img/scd2_ex.png)\n",
    "\n",
    "Here we have a **Products** dimension table which stores data from one or more source systems. As we have a type 2 SCD, every time a record is updated in source, a new record is added into the dimension table, with the updated version of the source record. There are various records for the same source entity, showing the 'condition' of the source record in various points in time, Historical information is retained this way. The primary key of the record in the source system (e.g. our ERP) is the **SKU** column (hence the NK - natural key designation). The extra columns we have in our SCD 2 Product Dim are the following:\n",
    "- **Product Key**: this is the PK (primary key) in the DIM Product table itself. As we have multiple rows for each original record in the source system, to retain the various versions, we can have multiple records with the same NK but with a different PK, of course. This PK has no intrisic meaning, hence it's also called a surrogate key, and it is an auto-increment field in most cases.\n",
    "- **Row Effective Date**: this shows when this particular record (so this version of the source record) was loaded from the source system. The first time we load the dimension table, it is customary to use a date in the distant past, like 1/1/1900 or something. \n",
    "- **Row Expiration Date**: this shows when the record/version stops being active. So every time an SCD2 attribute is changed in the source system, the expiration date of currently active record gets updated to the date (or datetime) of the current loading batch, and another record is added with Effective date having the same batch load date, and Expiration Date something like 12/31/9999, to indicate it's the currently active record.\n",
    "- **Current Row Indicator**: this is a flag that shows if the record is active or has expired. As an expiration date of 12/31/9999 shows the record is active too, sometimes this field is ommitted. It could also be used to indicate a record has been deleted from the source, or we could use another field for that.\n",
    "\n",
    "We can do the same thing in Delta Lake, although it's not necessary. To start with it, let's load a sample file, which will play the role of the source system. Now please note, that if you have installed Spark 3.0 and pyspark enabled Jupyter in your laptop, and want to try this there, you should kick this off using something like\n",
    "> pyspark --packages io.delta:delta-core_2.12:0.7.0 --conf \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\" --conf \"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\"\n",
    "\n",
    "as per the [documentation](https://docs.delta.io/latest/delta-batch.html#-sql-support) and the guys who wrote [Learning Spark 2nd ed.](https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/), in order to have the Delta Lake and SQL capabilities available in your Jupyter session. For Databricks Community ed. just import the notebook and upload the csv files and you're set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e8e33e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------+\n",
      "| ID|              Name|    City|\n",
      "+---+------------------+--------+\n",
      "|  1| John Papadopoulos|  Athens|\n",
      "|  2|   Matt Protopapas|  Athens|\n",
      "|  3|  Michael Georgiou|Salonica|\n",
      "+---+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#define source file\n",
    "sourceFile = \"files/customers1.csv\"\n",
    "\n",
    "#for Databricks Community Ed. use the below instead, after you've uploaded the file in an appropriate folder there\n",
    "#sourceFile =  \"dbfs:/FileStore/shared_uploads/youremail@yourprovider.com/customers1.csv\" \n",
    "\n",
    "# Configure Delta Lake table path\n",
    "deltaPath = \"/tmp/dim_customer\"\n",
    "\n",
    "#read file into spark data frame\n",
    "df1a = (spark.read.format(\"csv\")\n",
    ".option(\"header\", \"true\")\n",
    ".load(sourceFile))\n",
    "\n",
    "#show contents\n",
    "df1a.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e778ade",
   "metadata": {},
   "source": [
    "So we have a simple file showing our customers and the city they currently live. The primary key which uniquely defines a customer in our source system (the file) is **ID**. We now want to store this table in our Delta lake table **dim_customers** which will have an SCD Type 2 format. So we would like to have an auto-increment primary key along with the effective and exiration dates (we skip the 'current flag' for now). Given that auto-increments are not supported, as the data might reside in different partitions (and different computers in fact), we'll use monotonicaly increasing ID's instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "167cad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "df1b = (df1a\n",
    "      .withColumn(\"CustomerKey\", monotonically_increasing_id())\n",
    "      .withColumn(\"ValidFrom\",to_date(lit(\"01/01/1900\"), \"MM/dd/yyyy\"))\n",
    "      .withColumn(\"ValidTo\",to_date(lit(\"12/31/9999\"),\"MM/dd/yyyy\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38473316",
   "metadata": {},
   "source": [
    "Taditionally, the record with PK=0 in the dimension is a 'blank' record which is used to show the absence of a dimension entity. For example if the customer was not recorded for a sale, we would need to map the specific sale fact onto that 'blank' record in the dimension. It's not that difficult to add such a record in our data frame but we'll skip it for now. So, we're ready to write the first batch of the customer dimension into our table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "82163374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------+-----------+----------+----------+\n",
      "| ID|              Name|    City|CustomerKey| ValidFrom|   ValidTo|\n",
      "+---+------------------+--------+-----------+----------+----------+\n",
      "|  1| John Papadopoulos|  Athens|          0|1900-01-01|9999-12-31|\n",
      "|  2|   Matt Protopapas|  Athens|          1|1900-01-01|9999-12-31|\n",
      "|  3|  Michael Georgiou|Salonica|          2|1900-01-01|9999-12-31|\n",
      "+---+------------------+--------+-----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1b.write.format(\"delta\").saveAsTable('dimCustomer')\n",
    "#spark.sql(\"CREATE TABLE dimCustomer USING DELTA LOCATION 'home/matt/Code/modern-examples/pyspark'\")\n",
    "spark.sql(\"Select * from dimCustomer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef25e967",
   "metadata": {},
   "source": [
    "Now we can load the second batch. We'll assume the source table evolved to the state described in the file below, with one record added and one updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e0f5d41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------+---------------------------+\n",
      "| ID|              Name|    City|LastUpdatedCreatedTimestamp|\n",
      "+---+------------------+--------+---------------------------+\n",
      "|  1| John Papadopoulos|Salonica|        2021/08/29 00:57.12|\n",
      "|  2|   Matt Protopapas|  Athens|        2015/06/15 00:00.00|\n",
      "|  3|  Michael Georgiou|Salonica|        2019/04/12 12:55.34|\n",
      "|  4|       John Bishop|  Patras|        2021/08/30 10:00.00|\n",
      "+---+------------------+--------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#define source file\n",
    "sourceFile = \"files/customers2.csv\"\n",
    "\n",
    "#for Databricks Community Ed. use the below instead, after you've uploaded the file in an appropriate folder there\n",
    "#sourceFile =  \"dbfs:/FileStore/shared_uploads/youremail@yourprovider.com/customers2.csv\" \n",
    "\n",
    "#read file into spark data frame\n",
    "df2a = (spark.read.format(\"csv\")\n",
    ".option(\"header\", \"true\")\n",
    ".load(sourceFile))\n",
    "\n",
    "#show contents\n",
    "df2a.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18801a7",
   "metadata": {},
   "source": [
    "The extra column shows when the record was last updated (or inserted) and will be used later. John Papadopoulos moved to Salonica and John Bishop from Patras was added as well. We now want to add the ValidFrom, ValidTo columns as before, assuming a daily batch (which supposingly runs on 31Aug2021), but also to add the surrogate keys for the new records. Note that 2 new records should be added. One for the new John Bishop record and another one for the updated John Papadopoulos record. As the data are distributed to clusters, we need to get the max surrogate key already stored in all the data and ensure all new surrogate keys are greater than the max SK already there. Let's see how it can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3b46a862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------+---------------------------+-----------+----------+----------+\n",
      "| ID|              Name|    City|LastUpdatedCreatedTimestamp|CustomerKey| ValidFrom|   ValidTo|\n",
      "+---+------------------+--------+---------------------------+-----------+----------+----------+\n",
      "|  1| John Papadopoulos|Salonica|        2021/08/29 00:57.12|          3|2021-08-31|9999-12-31|\n",
      "|  2|   Matt Protopapas|  Athens|        2015/06/15 00:00.00|          4|2021-08-31|9999-12-31|\n",
      "|  3|  Michael Georgiou|Salonica|        2019/04/12 12:55.34|          5|2021-08-31|9999-12-31|\n",
      "|  4|       John Bishop|  Patras|        2021/08/30 10:00.00|          6|2021-08-31|9999-12-31|\n",
      "+---+------------------+--------+---------------------------+-----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get max SK as a bigint into maxSK variable\n",
    "maxSK=spark.sql(\"SELECT max(CustomerKey) as maxSK FROM dimCustomer\").collect()[0][\"maxSK\"]\n",
    "\n",
    "#Date the daily ETL is run\n",
    "ETLRefDate=lit(\"08/31/2021\")\n",
    "\n",
    "#create data frame of new source table state\n",
    "df2b = (df2a\n",
    "      .withColumn(\"CustomerKey\", monotonically_increasing_id()+maxSK+1)\n",
    "      .withColumn(\"ValidFrom\",to_date(ETLRefDate, \"MM/dd/yyyy\"))\n",
    "      .withColumn(\"ValidTo\",to_date(lit(\"12/31/9999\"),\"MM/dd/yyyy\")))\n",
    "\n",
    "####Probably the below is not needed\n",
    "#create a delta store and a temp view on top of it\n",
    "deltaPath2 = \"/tmp/updates\"\n",
    "df2b.write.format(\"delta\").save(deltaPath2)\n",
    "spark.read.format(\"delta\").load(deltaPath2).createOrReplaceTempView(\"updates\")\n",
    "\n",
    "#have a look at the data in the updates view\n",
    "spark.sql(\"SELECT * FROM updates\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "020c5d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------+-----------+----------+----------+\n",
      "| ID|              Name|    City|CustomerKey| ValidFrom|   ValidTo|\n",
      "+---+------------------+--------+-----------+----------+----------+\n",
      "|  1| John Papadopoulos|  Athens|          0|1900-01-01|2021-08-31|\n",
      "|  2|   Matt Protopapas|  Athens|          1|1900-01-01|9999-12-31|\n",
      "|  3|  Michael Georgiou|Salonica|          2|1900-01-01|9999-12-31|\n",
      "|  1| John Papadopoulos|Salonica|          3|2021-08-31|9999-12-31|\n",
      "|  4|       John Bishop|  Patras|          4|2021-08-31|9999-12-31|\n",
      "+---+------------------+--------+-----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#and now for the SCD2 merge\n",
    "\n",
    "spark.sql( \"\"\"\n",
    "  MERGE INTO dimCustomer USING (\n",
    "  \n",
    "    select updates.ID as mergeKey, updates.* \n",
    "    FROM updates\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    SELECT NULL as mergeKey, updates.*\n",
    "    FROM updates JOIN dimCustomer\n",
    "    on dimCustomer.ID=updates.ID\n",
    "    WHERE dimCustomer.ValidTo = '9999-12-31' AND (dimCustomer.Name<>updates.Name OR dimCustomer.City <> updates.City)\n",
    "  ) stg_upd\n",
    "  ON dimCustomer.ID = stg_upd.mergeKey\n",
    "  \n",
    "  \n",
    "  WHEN MATCHED AND dimCustomer.ValidTo = '9999-12-31'  AND (dimCustomer.Name<>stg_upd.Name OR dimCustomer.City <> stg_upd.City)\n",
    "   THEN UPDATE SET dimCustomer.ValidTo = stg_upd.ValidFrom    \n",
    "  WHEN NOT MATCHED\n",
    "    THEN INSERT(ID,Name,City,CustomerKey,ValidFrom,ValidTo) VALUES (ID,Name,City,CustomerKey,ValidFrom,ValidTo)\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"select * from dimCustomer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3137f774",
   "metadata": {},
   "source": [
    "The strange MERGE is due to Spark SQL not supporting INSERT clauses in the 'WHEN MATCHED' piece. So the first part updates the ValidTo of the current records that have now changed and the second part inserts both the new records and the new version of the changed records that are already in dimCustomer [check this for more info](https://docs.databricks.com/_static/notebooks/merge-in-scd-type-2.html). To be perfectly honest, I never got this merge to run on my laptop, as the stars, jars and config options would not align no matter how hard I tried. But then again, that's why we have the cloud...\n",
    "\n",
    "Also note that trying to get the max SK is quite sub-optimal, especially since in Spark (and big data workloads in general) mini-batches are preferred against one big daily batch. So, if we needed to get the max SK across the cluster each time we run a mini-batch (which could be a matter of seconds) then that would take most of the time / resources in the mini batch, which wouldn't be very efficient. \n",
    "\n",
    "In reality we don't need the surrogate keys at all, as we could join the facts to the dimension using the source system PK (in conjunction with a unique identifier for the source system if many were in scope) and the valid from/to columns. To do that within a mini-batch context, we'd rather have very accurrate valid from / to values, perhaps to the milisecond, not date. After all, this was common practice even in the old days of daily batches. Hence the use for the LastUpdatedCreatedTimestamp. Note that this was not needed in the initial load (csv 1) as there the columns were set with preset values.\n",
    "\n",
    "One might be tempted not to use those columns either, as DeltaLake provides for full history, along with a set of timestamps on when the record was inserted / updated in the Delta Table. However as those timestamps reflect the time the record was upserted in the spark data store and not the source system, we might have discrepanceis as fact tables are joined to dims that might be different at the time of the actual event in the source.\n",
    "\n",
    "So we'll proceed with creating another DIM_Customer table with the better structure and add the 'is_current' flag along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a23432fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "#define source csv 1\n",
    "sourceFile = \"files/customers1.csv\"\n",
    "\n",
    "#for Databricks Community Ed. use the below instead, after you've uploaded the file in an appropriate folder there\n",
    "#sourceFile =  \"dbfs:/FileStore/shared_uploads/youremail@yourprovider.com/customers1.csv\" \n",
    "\n",
    "#read csv 1 file into delta table\n",
    "(spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(sourceFile)\n",
    "    .withColumn(\"ValidFrom\",to_timestamp(lit(\"01/01/1900\"), \"MM/dd/yyyy\"))\n",
    "    .withColumn(\"ValidTo\",to_timestamp(lit(\"12/31/9999\"),\"MM/dd/yyyy\"))\n",
    "    .withColumn(\"is_current\",lit(1).cast('Boolean'))\n",
    "    .write.format(\"delta\").saveAsTable('DIM_Customer'))\n",
    "\n",
    "\n",
    "#define source csv 2\n",
    "sourceFile2 = \"files/customers2.csv\"\n",
    "\n",
    "#for Databricks Community Ed. use the below instead, after you've uploaded the file in an appropriate folder there\n",
    "#sourceFile2 =  \"dbfs:/FileStore/shared_uploads/youremail@yourprovider.com/customers2.csv\" \n",
    "\n",
    "#create temp view for csv 2\n",
    "(spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(sourceFile2)\n",
    "    .withColumn(\"LastUpdatedCreatedTimestamp\",to_timestamp(col(\"LastUpdatedCreatedTimestamp\"),\"yyyy/MM/dd HH:mm.ss\"))\n",
    "    .withColumn(\"ValidTo\",to_timestamp(lit(\"12/31/9999\"),\"MM/dd/yyyy\"))\n",
    "    .withColumn(\"is_current\",lit(1).cast('Boolean'))\n",
    "    .createOrReplaceTempView(\"updates_view\"))\n",
    "\n",
    "#merge\n",
    "spark.sql( \"\"\"\n",
    "  MERGE INTO DIM_CUSTOMER USING (\n",
    "  \n",
    "    select updates_view.ID as mergeKey, updates_view.* \n",
    "    FROM updates_view\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    SELECT NULL as mergeKey, updates_view.*\n",
    "    FROM updates_view JOIN DIM_Customer\n",
    "    on DIM_Customer.ID=updates_view.ID\n",
    "    WHERE DIM_Customer.is_current = true AND (DIM_Customer.Name<>updates_view.Name OR DIM_Customer.City <> updates_view.City)\n",
    "  ) stg_upd\n",
    "  ON DIM_Customer.ID = stg_upd.mergeKey\n",
    "  \n",
    "  \n",
    "  WHEN MATCHED AND DIM_Customer.is_current = true  AND (DIM_Customer.Name<>stg_upd.Name OR DIM_Customer.City <> stg_upd.City)\n",
    "   THEN UPDATE SET DIM_Customer.is_current = false, DIM_Customer.ValidTo = stg_upd.LastUpdatedCreatedTimestamp\n",
    "  WHEN NOT MATCHED\n",
    "    THEN INSERT(ID,Name,City,ValidFrom,ValidTo,is_current) VALUES (ID,Name,City,LastUpdatedCreatedTimestamp,ValidTo,is_current)\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
